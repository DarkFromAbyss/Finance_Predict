{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- 0. Chu·∫©n b·ªã Th∆∞ m·ª•c v√† Thi·∫øt b·ªã ---\n",
    "MODEL_DIR = 'models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Th∆∞ m·ª•c l∆∞u m√¥ h√¨nh ƒë√£ s·∫µn s√†ng: {MODEL_DIR}\")\n",
    "print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}\")\n",
    "\n",
    "# --- D·ªØ li·ªáu Gi·∫£ ƒë·ªãnh (Kh√¥ng Chu·∫©n h√≥a) ---\n",
    "N_SAMPLES = 1000\n",
    "N_FEATURES = 1 \n",
    "TIMESTEPS = 20 \n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu gi√° ti·ªÅn ·∫£o gi·∫£ ƒë·ªãnh c√≥ bi·∫øn ƒë·ªông l·ªõn\n",
    "np.random.seed(42)\n",
    "base_price = np.cumsum(np.random.randn(N_SAMPLES + TIMESTEPS) * 0.1) + 100\n",
    "# Th√™m c√°c 'd·ªã th∆∞·ªùng' (bi·∫øn ƒë·ªông l·ªõn) ng·∫´u nhi√™n\n",
    "anomaly_indices = np.random.randint(N_SAMPLES // 2, N_SAMPLES, size=10)\n",
    "base_price[anomaly_indices] += np.random.uniform(5, 10, size=10) # D·ªã th∆∞·ªùng l·ªõn\n",
    "raw_data = base_price.reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "# --- 1. H√†m Split Data v√† T·∫°o Sequence cho LSTM ---\n",
    "def create_sequences(data, timesteps):\n",
    "    \"\"\"T·∫°o X (sequence) v√† y (target) cho m√¥ h√¨nh LSTM.\"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(data) - timesteps):\n",
    "        # L·∫•y m·ªôt c·ª≠a s·ªï (window) d·ªØ li·ªáu\n",
    "        X = data[i:(i + timesteps), :]\n",
    "        # L·∫•y gi√° tr·ªã ti·∫øp theo l√†m target cho d·ª± ƒëo√°n (Model 1)\n",
    "        y_forecast = data[i + timesteps, :]\n",
    "        \n",
    "        Xs.append(X)\n",
    "        ys.append(y_forecast)\n",
    "        \n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Chia Train/Validation/Test (70/15/15)\n",
    "train_size = int(N_SAMPLES * 0.7)\n",
    "val_size = int(N_SAMPLES * 0.15)\n",
    "\n",
    "# Chia t·∫≠p d·ªØ li·ªáu\n",
    "train_data = raw_data[:train_size + TIMESTEPS]\n",
    "val_data = raw_data[train_size:train_size + val_size + TIMESTEPS]\n",
    "\n",
    "# T·∫°o sequences \n",
    "X_train_np, y_train_forecast_np = create_sequences(train_data, TIMESTEPS)\n",
    "X_val_np, y_val_forecast_np = create_sequences(val_data, TIMESTEPS)\n",
    "\n",
    "print(\"\\n--- K√≠ch th∆∞·ªõc D·ªØ li·ªáu sau khi chia (Kh√¥ng Chu·∫©n h√≥a) ---\")\n",
    "print(f\"X_train shape: {X_train_np.shape}\")\n",
    "print(f\"y_train_forecast shape: {y_train_forecast_np.shape}\")\n",
    "\n",
    "\n",
    "# --- 2. PyTorch Dataset v√† DataLoader ---\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Model 1 (D·ª± ƒëo√°n) DataLoaders\n",
    "train_dataset_forecast = TimeSeriesDataset(X_train_np, y_train_forecast_np)\n",
    "train_loader_forecast = DataLoader(train_dataset_forecast, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "val_dataset_forecast = TimeSeriesDataset(X_val_np, y_val_forecast_np)\n",
    "val_loader_forecast = DataLoader(val_dataset_forecast, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model 2 (Autoencoder) DataLoaders (Input v√† Target l√† X)\n",
    "train_dataset_anomaly = TimeSeriesDataset(X_train_np, X_train_np)\n",
    "train_loader_anomaly = DataLoader(train_dataset_anomaly, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "val_dataset_anomaly = TimeSeriesDataset(X_val_np, X_val_np)\n",
    "val_loader_anomaly = DataLoader(val_dataset_anomaly, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# --- 3. ƒê·ªãnh nghƒ©a c√°c M√¥ h√¨nh PyTorch LSTM ---\n",
    "\n",
    "## M√¥ h√¨nh 1: D·ª± ƒëo√°n Chu·ªói th·ªùi gian\n",
    "class LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMForecast, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Output, (h_n, c_n)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # L·∫•y output c·ªßa b∆∞·ªõc th·ªùi gian cu·ªëi c√πng\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "\n",
    "## M√¥ h√¨nh 2: Ph√°t hi·ªán B·∫•t th∆∞·ªùng (LSTM Autoencoder)\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.timesteps = TIMESTEPS\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, 2, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(hidden_size, input_size, 2, batch_first=True, dropout=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, timesteps, input_size)\n",
    "        \n",
    "        # 1. Encoder: H·ªçc c√°ch n√©n\n",
    "        # out_e: output cho t·ª´ng b∆∞·ªõc th·ªùi gian, (h_n, c_n): hidden/cell state cu·ªëi c√πng\n",
    "        _, (hidden_state, cell_state) = self.encoder(x)\n",
    "        \n",
    "        # 2. Decoder Input: L·∫•y hidden state cu·ªëi c√πng c·ªßa Encoder, nh√¢n b·∫£n cho m·ªçi b∆∞·ªõc th·ªùi gian\n",
    "        # K√≠ch th∆∞·ªõc hidden_state: (num_layers, batch_size, hidden_size)\n",
    "        # Ch√∫ng ta c·∫ßn hidden state c·ªßa layer cu·ªëi c√πng: hidden_state[-1, :, :]\n",
    "        decoder_input = hidden_state[-1, :, :].unsqueeze(1).repeat(1, self.timesteps, 1)\n",
    "        # decoder_input shape: (batch_size, timesteps, hidden_size)\n",
    "        \n",
    "        # 3. Decoder: H·ªçc c√°ch t√°i t·∫°o\n",
    "        # ·ªû ƒë√¢y ch√∫ng ta mu·ªën t√°i t·∫°o l·∫°i chu·ªói input ban ƒë·∫ßu. \n",
    "        # C·∫ßn ƒëi·ªÅu ch·ªânh ki·∫øn tr√∫c Decoder ho·∫∑c s·ª≠ d·ª•ng TimeDistributed.\n",
    "        # ƒê·ªÉ ƒë∆°n gi·∫£n v√† ph√π h·ª£p v·ªõi c√°ch Keras Autoencoder, ta d√πng hidden state l√†m input cho Decoder\n",
    "        \n",
    "        # Thay v√¨ decoder_input ph·ª©c t·∫°p, d√πng c√°ch ƒë∆°n gi·∫£n h∆°n:\n",
    "        # Gi·ªØ l·∫°i hidden state v√† cell state cu·ªëi c√πng c·ªßa Encoder\n",
    "        # S·ª≠ d·ª•ng ch√∫ng ƒë·ªÉ kh·ªüi t·∫°o Decoder\n",
    "        \n",
    "        # T·∫°o Tensor zero c√≥ k√≠ch th∆∞·ªõc (batch_size, timesteps, input_size)\n",
    "        # V√¨ decoder s·∫Ω output ra ch√≠nh input_size\n",
    "        decoder_input_tensor = torch.zeros(x.size(0), self.timesteps, self.encoder.hidden_size).to(x.device)\n",
    "\n",
    "        # Gi·ªØ l·∫°i (h_n, c_n) c·ªßa encoder\n",
    "        # S·ª≠ d·ª•ng output c·ªßa encoder l√†m ƒë·∫ßu v√†o cho Decoder. \n",
    "        # V√¨ Decoder c·∫ßn t√°i t·∫°o l·∫°i chu·ªói T b∆∞·ªõc, ta c·∫ßn m·ªôt b∆∞·ªõc trung gian.\n",
    "        \n",
    "        # C√°ch ti·∫øp c·∫≠n Autoencoder ƒë∆°n gi·∫£n:\n",
    "        # Output c·ªßa Encoder (hidden_state[-1]) -> RepeatVector -> Decoder (t√°i t·∫°o)\n",
    "        \n",
    "        # Input cho Decoder: hidden_state cu·ªëi c√πng (layer cu·ªëi, m·ªçi batch)\n",
    "        # shape: (batch_size, hidden_size)\n",
    "        latent_vector = hidden_state[-1]\n",
    "        \n",
    "        # L·∫∑p l·∫°i vector n√©n T l·∫ßn\n",
    "        decoder_input = latent_vector.unsqueeze(1).repeat(1, self.timesteps, 1)\n",
    "        \n",
    "        # Decoder: c·∫ßn LSTM v·ªõi hidden_size = input_size v√† output_size = input_size\n",
    "        # Ta s·∫Ω d√πng nn.Linear sau m·ªói b∆∞·ªõc c·ªßa decoder.\n",
    "        \n",
    "        # Kh·ªüi t·∫°o l·∫°i Decoder ƒë∆°n gi·∫£n h∆°n:\n",
    "        \n",
    "        out_dec, _ = self.decoder(decoder_input)\n",
    "        \n",
    "        # √Åp d·ª•ng Linear layer t·∫°i m·ªói b∆∞·ªõc th·ªùi gian\n",
    "        # V√¨ Output c·ªßa LSTM l√† (batch_size, timesteps, input_size)\n",
    "        # Ta c·∫ßn nn.Linear(input_size, N_FEATURES)\n",
    "        \n",
    "        final_output = nn.Linear(input_size, N_FEATURES).to(x.device)(out_dec)\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "\n",
    "# --- 4. H√†m Training (T√≠ch h·ª£p Early Stopping) ---\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, epochs, model_name):\n",
    "    print(f\"\\n#################################################################\")\n",
    "    print(f\"### B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN M√î H√åNH: {model_name} ###\")\n",
    "    print(f\"#################################################################\")\n",
    "    \n",
    "    # Early Stopping Parameters\n",
    "    patience = 10\n",
    "    min_delta = 1e-4\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    history = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # --- Training Loop ---\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            \n",
    "            loss = loss_fn(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # --- Validation Loop ---\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                \n",
    "                output = model(X_val)\n",
    "                val_loss = loss_fn(output, y_val)\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        history['loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "        # In ra loss (theo y√™u c·∫ßu)\n",
    "        print(f\"Epoch {epoch+1:02d} ({time.time() - start_time:.2f}s): Loss={avg_train_loss:.4f}, Val_Loss={avg_val_loss:.4f}\")\n",
    "\n",
    "        # --- Early Stopping Check ---\n",
    "        if avg_val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # L∆∞u checkpoint m√¥ h√¨nh t·ªët nh·∫•t\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_DIR, f'{model_name}_best_weights.pth'))\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"‚ö†Ô∏è Early stopping t·∫°i Epoch {epoch+1:02d}!\")\n",
    "            # T·∫£i l·∫°i tr·ªçng s·ªë t·ªët nh·∫•t tr∆∞·ªõc khi d·ª´ng\n",
    "            model.load_state_dict(torch.load(os.path.join(MODEL_DIR, f'{model_name}_best_weights.pth')))\n",
    "            break\n",
    "            \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# --- 5. Hu·∫•n luy·ªán M√¥ h√¨nh 1 (D·ª± ƒëo√°n) ---\n",
    "model_forecast = LSTMForecast(\n",
    "    input_size=N_FEATURES, \n",
    "    hidden_size=64, \n",
    "    num_layers=2, \n",
    "    output_size=N_FEATURES\n",
    ").to(device)\n",
    "\n",
    "loss_fn_forecast = nn.MSELoss() # D√πng MSE cho d·ª± ƒëo√°n gi√° tr·ªã li√™n t·ª•c\n",
    "optimizer_forecast = torch.optim.Adam(model_forecast.parameters(), lr=1e-3)\n",
    "\n",
    "model_forecast, history_forecast = train_model(\n",
    "    model_forecast, \n",
    "    train_loader_forecast, \n",
    "    val_loader_forecast, \n",
    "    loss_fn_forecast, \n",
    "    optimizer_forecast, \n",
    "    epochs=100, \n",
    "    model_name='lstm_forecast'\n",
    ")\n",
    "\n",
    "# L∆∞u m√¥ h√¨nh cu·ªëi c√πng (ƒë√£ kh√¥i ph·ª•c tr·ªçng s·ªë t·ªët nh·∫•t)\n",
    "torch.save(model_forecast.state_dict(), os.path.join(MODEL_DIR, 'lstm_forecast_model.pth'))\n",
    "print(f\"\\n‚úÖ M√¥ h√¨nh 1 ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {os.path.join(MODEL_DIR, 'lstm_forecast_model.pth')}\")\n",
    "\n",
    "\n",
    "# --- 6. Hu·∫•n luy·ªán M√¥ h√¨nh 2 (B·∫•t th∆∞·ªùng - Autoencoder) ---\n",
    "# PyTorch Autoencoder ph·ª©c t·∫°p h∆°n Keras, ta s·∫Ω ƒë∆°n gi·∫£n h√≥a ƒë·∫ßu ra c·ªßa Decoder ƒë·ªÉ ph√π h·ª£p.\n",
    "model_anomaly = LSTMAutoencoder(\n",
    "    input_size=N_FEATURES, \n",
    "    hidden_size=64 # hidden_size ph·∫£i ƒë·ªß l·ªõn ƒë·ªÉ ch·ª©a th√¥ng tin n√©n\n",
    ").to(device)\n",
    "\n",
    "loss_fn_anomaly = nn.L1Loss() # D√πng L1Loss (MAE) ƒë·ªÉ ƒë√°nh gi√° l·ªói t√°i t·∫°o\n",
    "optimizer_anomaly = torch.optim.Adam(model_anomaly.parameters(), lr=1e-3)\n",
    "\n",
    "model_anomaly, history_anomaly = train_model(\n",
    "    model_anomaly, \n",
    "    train_loader_anomaly, \n",
    "    val_loader_anomaly, \n",
    "    loss_fn_anomaly, \n",
    "    optimizer_anomaly, \n",
    "    epochs=100, \n",
    "    model_name='lstm_anomaly_autoencoder'\n",
    ")\n",
    "\n",
    "# L∆∞u m√¥ h√¨nh cu·ªëi c√πng (ƒë√£ kh√¥i ph·ª•c tr·ªçng s·ªë t·ªët nh·∫•t)\n",
    "torch.save(model_anomaly.state_dict(), os.path.join(MODEL_DIR, 'lstm_anomaly_autoencoder.pth'))\n",
    "print(f\"\\n‚úÖ M√¥ h√¨nh 2 ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {os.path.join(MODEL_DIR, 'lstm_anomaly_autoencoder.pth')}\")\n",
    "\n",
    "print(\"\\nüéâ Ho√†n t·∫•t vi·ªác x√¢y d·ª±ng, hu·∫•n luy·ªán v√† l∆∞u c·∫£ hai m√¥ h√¨nh b·∫±ng PyTorch!\")\n",
    "\n",
    "\n",
    "# --- Ph·∫ßn b·ªï sung: T·∫£i M√¥ h√¨nh ---\n",
    "print(\"\\n--- Ph·∫ßn b·ªï sung: T·∫£i m√¥ h√¨nh ƒë√£ l∆∞u ---\")\n",
    "\n",
    "# T·∫£i M√¥ h√¨nh 1 (D·ª± ƒëo√°n)\n",
    "loaded_model_forecast = LSTMForecast(N_FEATURES, 64, 2, N_FEATURES).to(device)\n",
    "loaded_model_forecast.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'lstm_forecast_model.pth')))\n",
    "loaded_model_forecast.eval()\n",
    "print(\"‚úÖ T·∫£i M√¥ h√¨nh 1 th√†nh c√¥ng.\")\n",
    "\n",
    "# T·∫£i M√¥ h√¨nh 2 (B·∫•t th∆∞·ªùng)\n",
    "loaded_model_anomaly = LSTMAutoencoder(N_FEATURES, 64).to(device)\n",
    "loaded_model_anomaly.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'lstm_anomaly_autoencoder.pth')))\n",
    "loaded_model_anomaly.eval()\n",
    "print(\"‚úÖ T·∫£i M√¥ h√¨nh 2 th√†nh c√¥ng.\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
